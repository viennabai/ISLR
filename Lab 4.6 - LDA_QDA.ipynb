{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis as QDA\n",
    "from sklearn.metrics import confusion_matrix, classification_report, precision_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.6.3 Linear Discriminant Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "Smarket = pd.read_csv('Smarket.csv', usecols=range(1,10), index_col=0, parse_dates=True)\n",
    "\n",
    "X_train = Smarket[:'2004'][['Lag1','Lag2']]\n",
    "y_train = Smarket[:'2004']['Direction']\n",
    "X_test = Smarket['2005':][['Lag1','Lag2']]\n",
    "y_test = Smarket['2005':]['Direction']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda = LDA()\n",
    "model = lda.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.49198397, 0.50801603])"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.priors_\n",
    "#49.2%of the training observations were when the market went up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.04279022,  0.03389409],\n",
       "       [-0.03954635, -0.03132544]])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.means_\n",
    "#tendency for previous days' returns to be positive on the days when market declines\n",
    "#tendancy for previous days' to be negative, on days when market increases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.05544078, -0.0443452 ]])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.coef_\n",
    "#coefficients differ from R"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array(['Down', 'Up'], dtype='<U4'), array([ 70, 182], dtype=int64))\n",
      "\n",
      "[[ 35  35]\n",
      " [ 76 106]]\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        Down      0.500     0.315     0.387       111\n",
      "          Up      0.582     0.752     0.656       141\n",
      "\n",
      "    accuracy                          0.560       252\n",
      "   macro avg      0.541     0.534     0.522       252\n",
      "weighted avg      0.546     0.560     0.538       252\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#predict Up or Down\n",
    "\n",
    "pred = model.predict(X_test)\n",
    "print(np.unique(pred, return_counts=True)) #predicted 70Down, 182 Up\n",
    "print()\n",
    "print(confusion_matrix(pred,y_test))\n",
    "print()\n",
    "print(classification_report(y_test, pred, digits=3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.49017925, 0.50982075],\n",
       "       [0.4792185 , 0.5207815 ],\n",
       "       [0.46681848, 0.53318152],\n",
       "       [0.47400107, 0.52599893],\n",
       "       [0.49278766, 0.50721234],\n",
       "       [0.49385615, 0.50614385],\n",
       "       [0.49510156, 0.50489844],\n",
       "       [0.4872861 , 0.5127139 ],\n",
       "       [0.49070135, 0.50929865],\n",
       "       [0.48440262, 0.51559738],\n",
       "       [0.49069628, 0.50930372],\n",
       "       [0.51199885, 0.48800115],\n",
       "       [0.48951523, 0.51048477],\n",
       "       [0.47067612, 0.52932388],\n",
       "       [0.47445929, 0.52554071],\n",
       "       [0.47995834, 0.52004166],\n",
       "       [0.49357753, 0.50642247],\n",
       "       [0.50308938, 0.49691062],\n",
       "       [0.49788061, 0.50211939],\n",
       "       [0.48863309, 0.51136691],\n",
       "       [0.50065681, 0.49934319],\n",
       "       [0.51087353, 0.48912647],\n",
       "       [0.50399248, 0.49600752],\n",
       "       [0.49163351, 0.50836649],\n",
       "       [0.50417721, 0.49582279],\n",
       "       [0.50267505, 0.49732495],\n",
       "       [0.49140429, 0.50859571],\n",
       "       [0.48059641, 0.51940359],\n",
       "       [0.48827181, 0.51172819],\n",
       "       [0.50621869, 0.49378131],\n",
       "       [0.50059958, 0.49940042],\n",
       "       [0.49729649, 0.50270351],\n",
       "       [0.49585462, 0.50414538],\n",
       "       [0.48117774, 0.51882226],\n",
       "       [0.48414175, 0.51585825],\n",
       "       [0.47263882, 0.52736118],\n",
       "       [0.48364175, 0.51635825],\n",
       "       [0.50910066, 0.49089934],\n",
       "       [0.51359414, 0.48640586],\n",
       "       [0.49338391, 0.50661609],\n",
       "       [0.49268564, 0.50731436],\n",
       "       [0.4978472 , 0.5021528 ],\n",
       "       [0.49209142, 0.50790858],\n",
       "       [0.50563459, 0.49436541],\n",
       "       [0.50622877, 0.49377123],\n",
       "       [0.48818939, 0.51181061],\n",
       "       [0.47252929, 0.52747071],\n",
       "       [0.48323391, 0.51676609],\n",
       "       [0.48350857, 0.51649143],\n",
       "       [0.49133344, 0.50866656],\n",
       "       [0.48775664, 0.51224336],\n",
       "       [0.47243859, 0.52756141],\n",
       "       [0.48548774, 0.51451226],\n",
       "       [0.49329107, 0.50670893],\n",
       "       [0.48459731, 0.51540269],\n",
       "       [0.47237179, 0.52762821],\n",
       "       [0.48161704, 0.51838296],\n",
       "       [0.49140673, 0.50859327],\n",
       "       [0.4942755 , 0.5057245 ],\n",
       "       [0.48412321, 0.51587679],\n",
       "       [0.50260644, 0.49739356],\n",
       "       [0.50625572, 0.49374428],\n",
       "       [0.48218003, 0.51781997],\n",
       "       [0.48852631, 0.51147369],\n",
       "       [0.50118249, 0.49881751],\n",
       "       [0.50005949, 0.49994051],\n",
       "       [0.50273766, 0.49726234],\n",
       "       [0.48700861, 0.51299139],\n",
       "       [0.48272133, 0.51727867],\n",
       "       [0.49965006, 0.50034994],\n",
       "       [0.4818079 , 0.5181921 ],\n",
       "       [0.4651057 , 0.5348943 ],\n",
       "       [0.45778674, 0.54221326],\n",
       "       [0.47750037, 0.52249963],\n",
       "       [0.50342498, 0.49657502],\n",
       "       [0.48016639, 0.51983361],\n",
       "       [0.50461711, 0.49538289],\n",
       "       [0.50447517, 0.49552483],\n",
       "       [0.4964663 , 0.5035337 ],\n",
       "       [0.48929652, 0.51070348],\n",
       "       [0.48762358, 0.51237642],\n",
       "       [0.48056255, 0.51943745],\n",
       "       [0.4958518 , 0.5041482 ],\n",
       "       [0.51152122, 0.48847878],\n",
       "       [0.49585715, 0.50414285],\n",
       "       [0.50828713, 0.49171287],\n",
       "       [0.50220909, 0.49779091],\n",
       "       [0.48758917, 0.51241083],\n",
       "       [0.49959482, 0.50040518],\n",
       "       [0.48419171, 0.51580829],\n",
       "       [0.48588431, 0.51411569],\n",
       "       [0.48269686, 0.51730314],\n",
       "       [0.47450117, 0.52549883],\n",
       "       [0.50085397, 0.49914603],\n",
       "       [0.51277655, 0.48722345],\n",
       "       [0.51354723, 0.48645277],\n",
       "       [0.50951274, 0.49048726],\n",
       "       [0.49502005, 0.50497995],\n",
       "       [0.49560882, 0.50439118],\n",
       "       [0.49646433, 0.50353567],\n",
       "       [0.48743629, 0.51256371],\n",
       "       [0.49703392, 0.50296608],\n",
       "       [0.50037515, 0.49962485],\n",
       "       [0.48461365, 0.51538635],\n",
       "       [0.49769137, 0.50230863],\n",
       "       [0.50430808, 0.49569192],\n",
       "       [0.48433658, 0.51566342],\n",
       "       [0.48606641, 0.51393359],\n",
       "       [0.49304173, 0.50695827],\n",
       "       [0.48872186, 0.51127814],\n",
       "       [0.49681471, 0.50318529],\n",
       "       [0.49449886, 0.50550114],\n",
       "       [0.49247425, 0.50752575],\n",
       "       [0.49801415, 0.50198585],\n",
       "       [0.49787272, 0.50212728],\n",
       "       [0.49943897, 0.50056103],\n",
       "       [0.50283166, 0.49716834],\n",
       "       [0.49645027, 0.50354973],\n",
       "       [0.48832022, 0.51167978],\n",
       "       [0.48998014, 0.51001986],\n",
       "       [0.4771957 , 0.5228043 ],\n",
       "       [0.46940305, 0.53059695],\n",
       "       [0.48246925, 0.51753075],\n",
       "       [0.50379432, 0.49620568],\n",
       "       [0.50009743, 0.49990257],\n",
       "       [0.48053033, 0.51946967],\n",
       "       [0.48769528, 0.51230472],\n",
       "       [0.50707817, 0.49292183],\n",
       "       [0.49017763, 0.50982237],\n",
       "       [0.48609992, 0.51390008],\n",
       "       [0.51084971, 0.48915029],\n",
       "       [0.51355466, 0.48644534],\n",
       "       [0.50202175, 0.49797825],\n",
       "       [0.49568296, 0.50431704],\n",
       "       [0.49655358, 0.50344642],\n",
       "       [0.49645901, 0.50354099],\n",
       "       [0.48557189, 0.51442811],\n",
       "       [0.4951439 , 0.5048561 ],\n",
       "       [0.50600481, 0.49399519],\n",
       "       [0.48806432, 0.51193568],\n",
       "       [0.49211754, 0.50788246],\n",
       "       [0.49271947, 0.50728053],\n",
       "       [0.49016611, 0.50983389],\n",
       "       [0.5001986 , 0.4998014 ],\n",
       "       [0.50477457, 0.49522543],\n",
       "       [0.48752671, 0.51247329],\n",
       "       [0.48476481, 0.51523519],\n",
       "       [0.50284047, 0.49715953],\n",
       "       [0.50084349, 0.49915651],\n",
       "       [0.48255906, 0.51744094],\n",
       "       [0.47321244, 0.52678756],\n",
       "       [0.47977314, 0.52022686],\n",
       "       [0.49831721, 0.50168279],\n",
       "       [0.49688235, 0.50311765],\n",
       "       [0.49970307, 0.50029693],\n",
       "       [0.49147206, 0.50852794],\n",
       "       [0.48922997, 0.51077003],\n",
       "       [0.47876945, 0.52123055],\n",
       "       [0.47992336, 0.52007664],\n",
       "       [0.49138178, 0.50861822],\n",
       "       [0.49162875, 0.50837125],\n",
       "       [0.49487945, 0.50512055],\n",
       "       [0.48909001, 0.51090999],\n",
       "       [0.47909435, 0.52090565],\n",
       "       [0.4878531 , 0.5121469 ],\n",
       "       [0.48618381, 0.51381619],\n",
       "       [0.49355582, 0.50644418],\n",
       "       [0.49413286, 0.50586714],\n",
       "       [0.50207617, 0.49792383],\n",
       "       [0.50430515, 0.49569485],\n",
       "       [0.48904303, 0.51095697],\n",
       "       [0.50620061, 0.49379939],\n",
       "       [0.50927672, 0.49072328],\n",
       "       [0.48936695, 0.51063305],\n",
       "       [0.49877757, 0.50122243],\n",
       "       [0.4997456 , 0.5002544 ],\n",
       "       [0.48068521, 0.51931479],\n",
       "       [0.47905361, 0.52094639],\n",
       "       [0.48894962, 0.51105038],\n",
       "       [0.50394655, 0.49605345],\n",
       "       [0.49341736, 0.50658264],\n",
       "       [0.4748985 , 0.5251015 ],\n",
       "       [0.4706261 , 0.5293739 ],\n",
       "       [0.48689783, 0.51310217],\n",
       "       [0.49675542, 0.50324458],\n",
       "       [0.49294486, 0.50705514],\n",
       "       [0.49228531, 0.50771469],\n",
       "       [0.493369  , 0.506631  ],\n",
       "       [0.50536007, 0.49463993],\n",
       "       [0.50305521, 0.49694479],\n",
       "       [0.49058366, 0.50941634],\n",
       "       [0.47623902, 0.52376098],\n",
       "       [0.46033919, 0.53966081],\n",
       "       [0.46979321, 0.53020679],\n",
       "       [0.49253001, 0.50746999],\n",
       "       [0.48611431, 0.51388569],\n",
       "       [0.48113758, 0.51886242],\n",
       "       [0.48124736, 0.51875264],\n",
       "       [0.48423833, 0.51576167],\n",
       "       [0.50262179, 0.49737821],\n",
       "       [0.50523122, 0.49476878],\n",
       "       [0.4813184 , 0.5186816 ],\n",
       "       [0.50153968, 0.49846032],\n",
       "       [0.48771613, 0.51228387],\n",
       "       [0.47741706, 0.52258294],\n",
       "       [0.51688267, 0.48311733],\n",
       "       [0.507264  , 0.492736  ],\n",
       "       [0.48335152, 0.51664848],\n",
       "       [0.47267015, 0.52732985],\n",
       "       [0.5032667 , 0.4967333 ],\n",
       "       [0.52023495, 0.47976505],\n",
       "       [0.4950279 , 0.5049721 ],\n",
       "       [0.50187665, 0.49812335],\n",
       "       [0.50891419, 0.49108581],\n",
       "       [0.49689113, 0.50310887],\n",
       "       [0.49515948, 0.50484052],\n",
       "       [0.4895942 , 0.5104058 ],\n",
       "       [0.49046532, 0.50953468],\n",
       "       [0.50553179, 0.49446821],\n",
       "       [0.50554162, 0.49445838],\n",
       "       [0.49424704, 0.50575296],\n",
       "       [0.48574952, 0.51425048],\n",
       "       [0.49016058, 0.50983942],\n",
       "       [0.506973  , 0.493027  ],\n",
       "       [0.50847644, 0.49152356],\n",
       "       [0.50412876, 0.49587124],\n",
       "       [0.50482987, 0.49517013],\n",
       "       [0.50238787, 0.49761213],\n",
       "       [0.49869029, 0.50130971],\n",
       "       [0.48247575, 0.51752425],\n",
       "       [0.48254694, 0.51745306],\n",
       "       [0.48316002, 0.51683998],\n",
       "       [0.50174966, 0.49825034],\n",
       "       [0.50587076, 0.49412924],\n",
       "       [0.48903208, 0.51096792],\n",
       "       [0.49110524, 0.50889476],\n",
       "       [0.48642499, 0.51357501],\n",
       "       [0.48470615, 0.51529385],\n",
       "       [0.49448897, 0.50551103],\n",
       "       [0.49622614, 0.50377386],\n",
       "       [0.50057022, 0.49942978],\n",
       "       [0.5039068 , 0.4960932 ],\n",
       "       [0.49463764, 0.50536236],\n",
       "       [0.48643657, 0.51356343],\n",
       "       [0.4807022 , 0.5192978 ],\n",
       "       [0.48514389, 0.51485611],\n",
       "       [0.49517341, 0.50482659],\n",
       "       [0.50058931, 0.49941069],\n",
       "       [0.497221  , 0.502779  ],\n",
       "       [0.4791988 , 0.5208012 ],\n",
       "       [0.48316733, 0.51683267],\n",
       "       [0.4892591 , 0.5107409 ]])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#prediction probabilities \n",
    "\n",
    "pred_p = model.predict_proba(X_test)\n",
    "pred_p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([False,  True]), array([ 70, 182], dtype=int64))"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(pred_p[:,1]>0.5, return_counts=True)\n",
    "\n",
    "#this is the default - same as above "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([['0.5093037238790318', 'Up'],\n",
       "       ['0.4880011537380811', 'Down'],\n",
       "       ['0.510484773063352', 'Up'],\n",
       "       ['0.5293238777881214', 'Up'],\n",
       "       ['0.5255407143881711', 'Up'],\n",
       "       ['0.5200416608518921', 'Up'],\n",
       "       ['0.5064224705341396', 'Up'],\n",
       "       ['0.4969106228816935', 'Down'],\n",
       "       ['0.5021193878585957', 'Up'],\n",
       "       ['0.5113669134834818', 'Up']], dtype='<U32')"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.stack((pred_p[10:20,1], pred[10:20])).T\n",
    "\n",
    "#these probabilities output by the model corresponds to the probability that that market will go UP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Changing the threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([False]), array([252], dtype=int64))\n"
     ]
    }
   ],
   "source": [
    "print(np.unique(pred_p[:,1]>0.9, return_counts=True))\n",
    "\n",
    "#we want to prodict a market decrease ONLY if posterior proability is at least 90% \n",
    "# --- no days in 2005 meet that threshold "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5422132554518978"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#the greatest posterior probability of decrease in 2005 was: \n",
    "max(pred_p[:,1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.6.4 Quadratic Discriminant Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "qda = QDA()\n",
    "model2 = qda.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.49198397 0.50801603]\n",
      "[[ 0.04279022  0.03389409]\n",
      " [-0.03954635 -0.03132544]]\n"
     ]
    }
   ],
   "source": [
    "print(model2.priors_)\n",
    "print(model2.means_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array(['Down', 'Up'], dtype=object), array([ 50, 202], dtype=int64))\n",
      "[[ 30  20]\n",
      " [ 81 121]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        Down      0.600     0.270     0.373       111\n",
      "          Up      0.599     0.858     0.706       141\n",
      "\n",
      "    accuracy                          0.599       252\n",
      "   macro avg      0.600     0.564     0.539       252\n",
      "weighted avg      0.599     0.599     0.559       252\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pred2 = model2.predict(X_test)\n",
    "print(np.unique(pred2, return_counts=True))\n",
    "print(confusion_matrix(pred2, y_test))\n",
    "print(classification_report(y_test, pred2, digits=3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## An Application to Carseats Data???"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Sales  CompPrice  Income  Advertising  Population  Price  Age  Education  \\\n",
      "0   9.50        138      73           11         276    120   42         17   \n",
      "1  11.22        111      48           16         260     83   65         10   \n",
      "2  10.06        113      35           10         269     80   59         12   \n",
      "3   7.40        117     100            4         466     97   55         14   \n",
      "4   4.15        141      64            3         340    128   38         13   \n",
      "\n",
      "  Urban   US  ShelveLoc_Bad  ShelveLoc_Good  ShelveLoc_Medium  \n",
      "0   Yes  Yes              1               0                 0  \n",
      "1   Yes  Yes              0               1                 0  \n",
      "2   Yes  Yes              0               0                 1  \n",
      "3   Yes  Yes              0               0                 1  \n",
      "4   Yes   No              1               0                 0  \n",
      "(400, 13)\n",
      "Index(['Sales', 'CompPrice', 'Income', 'Advertising', 'Population', 'Price',\n",
      "       'Age', 'Education', 'Urban', 'US', 'ShelveLoc_Bad', 'ShelveLoc_Good',\n",
      "       'ShelveLoc_Medium'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "df2 = pd.read_csv('Carseats.csv')\n",
    "df2 = pd.get_dummies(df2, drop_first=False, columns=['ShelveLoc'])\n",
    "\n",
    "print(df2.head())\n",
    "print(df2.shape)\n",
    "print(df2.columns)\n",
    "\n",
    "X_train = df2.loc[:319,'Sales':'Education']\n",
    "y_train_bad = df2.loc[:319,'ShelveLoc_Bad']\n",
    "y_train_good = df2.loc[:319,'ShelveLoc_Good']\n",
    "y_train_medium = df2.loc[:319,'ShelveLoc_Medium']\n",
    "\n",
    "X_test = df2.loc[320:,'Sales':'Education']\n",
    "y_test_bad = df2.loc[320:,'ShelveLoc_Bad']\n",
    "y_test_good = df2.loc[320:,'ShelveLoc_Good']\n",
    "y_test_medium = df2.loc[320:,'ShelveLoc_Medium']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([4.73656651e-04, 4.80696661e-04, 6.19421571e-04, 8.03844206e-04,\n",
      "       8.94722899e-04, 9.76682569e-04, 1.05497540e-03, 1.11166565e-03,\n",
      "       1.15184580e-03, 1.17247201e-03, 1.18029478e-03, 1.18632760e-03,\n",
      "       1.67843342e-03, 1.79958728e-03, 1.82168677e-03, 1.91364501e-03,\n",
      "       1.97304885e-03, 2.07333144e-03, 2.25304478e-03, 2.33320173e-03,\n",
      "       2.64228926e-03, 2.64380835e-03, 2.74581553e-03, 2.75889308e-03,\n",
      "       2.79164640e-03, 2.79596491e-03, 3.01588054e-03, 3.03263050e-03,\n",
      "       3.34721004e-03, 3.91072368e-03, 4.23790064e-03, 4.24521697e-03,\n",
      "       4.36536683e-03, 4.56216788e-03, 5.46575855e-03, 5.76584682e-03,\n",
      "       5.76811406e-03, 6.20098580e-03, 6.20689930e-03, 6.32953915e-03,\n",
      "       6.39150154e-03, 6.52847438e-03, 7.10447980e-03, 7.85451167e-03,\n",
      "       7.87525706e-03, 7.94009141e-03, 8.01702820e-03, 8.66455538e-03,\n",
      "       8.81568635e-03, 9.57452793e-03, 9.72638886e-03, 1.01983609e-02,\n",
      "       1.04897127e-02, 1.17158803e-02, 1.22883141e-02, 1.24378583e-02,\n",
      "       1.26639207e-02, 1.54901468e-02, 1.61592522e-02, 1.81164278e-02,\n",
      "       1.92625077e-02, 1.96402385e-02, 1.96620634e-02, 1.98373943e-02,\n",
      "       2.18799007e-02, 2.33095626e-02, 2.35903983e-02, 2.48852745e-02,\n",
      "       2.52060696e-02, 2.52670170e-02, 2.57365426e-02, 2.69051742e-02,\n",
      "       2.70206816e-02, 2.73843880e-02, 2.88993273e-02, 3.04898229e-02,\n",
      "       3.05609545e-02, 3.06779530e-02, 3.08439019e-02, 3.12025970e-02,\n",
      "       3.14228485e-02, 3.28684407e-02, 3.34006906e-02, 3.51701967e-02,\n",
      "       3.57599805e-02, 3.66442104e-02, 3.83200969e-02, 4.09278138e-02,\n",
      "       4.42972078e-02, 4.53832572e-02, 4.55369518e-02, 4.69227534e-02,\n",
      "       4.71871160e-02, 4.74790034e-02, 4.93169034e-02, 4.94740821e-02,\n",
      "       5.02394863e-02, 5.07447516e-02, 5.23170954e-02, 5.42006599e-02,\n",
      "       5.42401483e-02, 5.51139854e-02, 5.73195590e-02, 5.86048429e-02,\n",
      "       5.94047069e-02, 6.40834374e-02, 6.46361412e-02, 6.50735387e-02,\n",
      "       6.51448786e-02, 6.67985836e-02, 6.79268901e-02, 6.81161397e-02,\n",
      "       6.91801059e-02, 6.95887777e-02, 7.02079243e-02, 7.10563226e-02,\n",
      "       7.22363079e-02, 7.25587303e-02, 7.39480215e-02, 7.46237914e-02,\n",
      "       7.63647514e-02, 7.63673808e-02, 7.64280882e-02, 7.74715693e-02,\n",
      "       7.79469181e-02, 7.89862972e-02, 8.10989178e-02, 8.36235020e-02,\n",
      "       8.37220748e-02, 8.43932131e-02, 8.47109367e-02, 8.66488205e-02,\n",
      "       8.78838526e-02, 8.85427981e-02, 8.88902128e-02, 8.92210690e-02,\n",
      "       8.92475463e-02, 8.98509314e-02, 9.05708184e-02, 9.63062021e-02,\n",
      "       9.69362153e-02, 9.78757605e-02, 9.81916243e-02, 9.98650252e-02,\n",
      "       1.03434205e-01, 1.06058700e-01, 1.06213921e-01, 1.07375970e-01,\n",
      "       1.08292558e-01, 1.08586741e-01, 1.08845025e-01, 1.09061061e-01,\n",
      "       1.09365070e-01, 1.09652311e-01, 1.15360182e-01, 1.16121739e-01,\n",
      "       1.16462505e-01, 1.16925133e-01, 1.17398854e-01, 1.18158932e-01,\n",
      "       1.20370236e-01, 1.21320768e-01, 1.24670809e-01, 1.26840746e-01,\n",
      "       1.27049428e-01, 1.29139410e-01, 1.29310461e-01, 1.30946684e-01,\n",
      "       1.33410038e-01, 1.33420606e-01, 1.33536413e-01, 1.36955850e-01,\n",
      "       1.37232023e-01, 1.40449554e-01, 1.40950394e-01, 1.42623772e-01,\n",
      "       1.42679418e-01, 1.43245642e-01, 1.43648325e-01, 1.45155613e-01,\n",
      "       1.46233285e-01, 1.49359009e-01, 1.49526954e-01, 1.50084195e-01,\n",
      "       1.51041262e-01, 1.52429397e-01, 1.53286359e-01, 1.53399432e-01,\n",
      "       1.61047809e-01, 1.61837332e-01, 1.64930257e-01, 1.66013733e-01,\n",
      "       1.66380371e-01, 1.66418315e-01, 1.67031422e-01, 1.67327415e-01,\n",
      "       1.67513778e-01, 1.75347674e-01, 1.75557705e-01, 1.82750628e-01,\n",
      "       1.84058890e-01, 1.84430850e-01, 1.85532086e-01, 1.86748387e-01,\n",
      "       1.88030608e-01, 1.89582112e-01, 1.92093197e-01, 1.92956558e-01,\n",
      "       1.98322692e-01, 2.00112510e-01, 2.01142948e-01, 2.01995729e-01,\n",
      "       2.05373274e-01, 2.05426944e-01, 2.06488670e-01, 2.06549329e-01,\n",
      "       2.07572547e-01, 2.09923623e-01, 2.12807986e-01, 2.16748206e-01,\n",
      "       2.17276060e-01, 2.22130318e-01, 2.26764784e-01, 2.27997800e-01,\n",
      "       2.29186220e-01, 2.36252081e-01, 2.38028549e-01, 2.38793389e-01,\n",
      "       2.40976858e-01, 2.41596585e-01, 2.41874447e-01, 2.42827282e-01,\n",
      "       2.43745143e-01, 2.43789209e-01, 2.45753842e-01, 2.47270036e-01,\n",
      "       2.50430793e-01, 2.50493875e-01, 2.51756380e-01, 2.58763702e-01,\n",
      "       2.59278602e-01, 2.60439192e-01, 2.61661785e-01, 2.64339169e-01,\n",
      "       2.71994967e-01, 2.73548108e-01, 2.74903824e-01, 2.75883077e-01,\n",
      "       2.77094191e-01, 2.81673398e-01, 2.83562463e-01, 2.85162574e-01,\n",
      "       2.87418636e-01, 2.94835215e-01, 2.95249644e-01, 3.00378025e-01,\n",
      "       3.02154470e-01, 3.06228115e-01, 3.13233144e-01, 3.14798850e-01,\n",
      "       3.15158814e-01, 3.20766131e-01, 3.21992216e-01, 3.27902962e-01,\n",
      "       3.28022921e-01, 3.39552114e-01, 3.41310110e-01, 3.41744848e-01,\n",
      "       3.49599281e-01, 3.49637089e-01, 3.50510631e-01, 3.54733728e-01,\n",
      "       3.56812680e-01, 3.62708942e-01, 3.68809035e-01, 3.76225215e-01,\n",
      "       3.81560288e-01, 3.84460491e-01, 4.00593353e-01, 4.01445264e-01,\n",
      "       4.08283518e-01, 4.18714330e-01, 4.19003035e-01, 4.19480611e-01,\n",
      "       4.21983752e-01, 4.23858900e-01, 4.24368894e-01, 4.29883118e-01,\n",
      "       4.34433221e-01, 4.35387950e-01, 4.38061987e-01, 4.40558221e-01,\n",
      "       4.41140793e-01, 4.41286908e-01, 4.42324670e-01, 4.43394722e-01,\n",
      "       4.45502315e-01, 4.46148924e-01, 4.48990378e-01, 4.50338371e-01,\n",
      "       4.51156892e-01, 4.52762901e-01, 4.53120904e-01, 4.54358528e-01,\n",
      "       4.63424106e-01, 4.64174484e-01, 4.67216166e-01, 4.69804158e-01,\n",
      "       4.70538847e-01, 4.72855373e-01, 4.74068892e-01, 4.75573749e-01,\n",
      "       4.76156136e-01, 4.79009382e-01, 4.79936563e-01, 4.88856520e-01,\n",
      "       4.89662342e-01, 4.91096861e-01, 4.92948639e-01, 4.97869941e-01,\n",
      "       5.02130059e-01, 5.07051361e-01, 5.08903139e-01, 5.10337658e-01,\n",
      "       5.11143480e-01, 5.20063437e-01, 5.20990618e-01, 5.23843864e-01,\n",
      "       5.24426251e-01, 5.25931108e-01, 5.27144627e-01, 5.29461153e-01,\n",
      "       5.30195842e-01, 5.32783834e-01, 5.35825516e-01, 5.36575894e-01,\n",
      "       5.45641472e-01, 5.46879096e-01, 5.47237099e-01, 5.48843108e-01,\n",
      "       5.49661629e-01, 5.51009622e-01, 5.53851076e-01, 5.54497685e-01,\n",
      "       5.56605278e-01, 5.57675330e-01, 5.58713092e-01, 5.58859207e-01,\n",
      "       5.59441779e-01, 5.61938013e-01, 5.64612050e-01, 5.65566779e-01,\n",
      "       5.70116882e-01, 5.75631106e-01, 5.76141100e-01, 5.78016248e-01,\n",
      "       5.80519389e-01, 5.80996965e-01, 5.81285670e-01, 5.91716482e-01,\n",
      "       5.98554736e-01, 5.99406647e-01, 6.15539509e-01, 6.18439712e-01,\n",
      "       6.23774785e-01, 6.31190965e-01, 6.37291058e-01, 6.43187320e-01,\n",
      "       6.45266272e-01, 6.49489369e-01, 6.50362911e-01, 6.50400719e-01,\n",
      "       6.58255152e-01, 6.58689890e-01, 6.60447886e-01, 6.71977079e-01,\n",
      "       6.72097038e-01, 6.78007784e-01, 6.79233869e-01, 6.84841186e-01,\n",
      "       6.85201150e-01, 6.86766856e-01, 6.93771885e-01, 6.97845530e-01,\n",
      "       6.99621975e-01, 7.04750356e-01, 7.05164785e-01, 7.12581364e-01,\n",
      "       7.14837426e-01, 7.16437537e-01, 7.18326602e-01, 7.22905809e-01,\n",
      "       7.24116923e-01, 7.25096176e-01, 7.26451892e-01, 7.28005033e-01,\n",
      "       7.35660831e-01, 7.38338215e-01, 7.39560808e-01, 7.40721398e-01,\n",
      "       7.41236298e-01, 7.48243620e-01, 7.49506125e-01, 7.49569207e-01,\n",
      "       7.52729964e-01, 7.54246158e-01, 7.56210791e-01, 7.56254857e-01,\n",
      "       7.57172718e-01, 7.58125553e-01, 7.58403415e-01, 7.59023142e-01,\n",
      "       7.61206611e-01, 7.61971451e-01, 7.63747919e-01, 7.70813780e-01,\n",
      "       7.72002200e-01, 7.73235216e-01, 7.77869682e-01, 7.82723940e-01,\n",
      "       7.83251794e-01, 7.87192014e-01, 7.90076377e-01, 7.92427453e-01,\n",
      "       7.93450671e-01, 7.93511330e-01, 7.94573056e-01, 7.94626726e-01,\n",
      "       7.98004271e-01, 7.98857052e-01, 7.99887490e-01, 8.01677308e-01,\n",
      "       8.07043442e-01, 8.07906803e-01, 8.10417888e-01, 8.11969392e-01,\n",
      "       8.13251613e-01, 8.14467914e-01, 8.15569150e-01, 8.15941110e-01,\n",
      "       8.17249372e-01, 8.24442295e-01, 8.24652326e-01, 8.32486222e-01,\n",
      "       8.32672585e-01, 8.32968578e-01, 8.33581685e-01, 8.33619629e-01,\n",
      "       8.33986267e-01, 8.35069743e-01, 8.38162668e-01, 8.38952191e-01,\n",
      "       8.46600568e-01, 8.46713641e-01, 8.47570603e-01, 8.48958738e-01,\n",
      "       8.49915805e-01, 8.50473046e-01, 8.50640991e-01, 8.53766715e-01,\n",
      "       8.54844387e-01, 8.56351675e-01, 8.56754358e-01, 8.57320582e-01,\n",
      "       8.57376228e-01, 8.59049606e-01, 8.59550446e-01, 8.62767977e-01,\n",
      "       8.63044150e-01, 8.66463587e-01, 8.66579394e-01, 8.66589962e-01,\n",
      "       8.69053316e-01, 8.70689539e-01, 8.70860590e-01, 8.72950572e-01,\n",
      "       8.73159254e-01, 8.75329191e-01, 8.78679232e-01, 8.79629764e-01,\n",
      "       8.81841068e-01, 8.82601146e-01, 8.83074867e-01, 8.83537495e-01,\n",
      "       8.83878261e-01, 8.84639818e-01, 8.90347689e-01, 8.90634930e-01,\n",
      "       8.90938939e-01, 8.91154975e-01, 8.91413259e-01, 8.91707442e-01,\n",
      "       8.92624030e-01, 8.93786079e-01, 8.93941300e-01, 8.96565795e-01,\n",
      "       9.00134975e-01, 9.01808376e-01, 9.02124239e-01, 9.03063785e-01,\n",
      "       9.03693798e-01, 9.09429182e-01, 9.10149069e-01, 9.10752454e-01,\n",
      "       9.10778931e-01, 9.11109787e-01, 9.11457202e-01, 9.12116147e-01,\n",
      "       9.13351179e-01, 9.15289063e-01, 9.15606787e-01, 9.16277925e-01,\n",
      "       9.16376498e-01, 9.18901082e-01, 9.21013703e-01, 9.22053082e-01,\n",
      "       9.22528431e-01, 9.23571912e-01, 9.23632619e-01, 9.23635249e-01,\n",
      "       9.25376209e-01, 9.26051979e-01, 9.27441270e-01, 9.27763692e-01,\n",
      "       9.28943677e-01, 9.29792076e-01, 9.30411222e-01, 9.30819894e-01,\n",
      "       9.31883860e-01, 9.32073110e-01, 9.33201416e-01, 9.34855121e-01,\n",
      "       9.34926461e-01, 9.35363859e-01, 9.35916563e-01, 9.40595293e-01,\n",
      "       9.41395157e-01, 9.42680441e-01, 9.44886015e-01, 9.45759852e-01,\n",
      "       9.45799340e-01, 9.47682905e-01, 9.49255248e-01, 9.49760514e-01,\n",
      "       9.50525918e-01, 9.50683097e-01, 9.52520997e-01, 9.52812884e-01,\n",
      "       9.53077247e-01, 9.54463048e-01, 9.54616743e-01, 9.55702792e-01,\n",
      "       9.59072186e-01, 9.61679903e-01, 9.63355790e-01, 9.64240019e-01,\n",
      "       9.64829803e-01, 9.66599309e-01, 9.67131559e-01, 9.68577151e-01,\n",
      "       9.68797403e-01, 9.69156098e-01, 9.69322047e-01, 9.69439045e-01,\n",
      "       9.69510177e-01, 9.71100673e-01, 9.72615612e-01, 9.72979318e-01,\n",
      "       9.73094826e-01, 9.74263457e-01, 9.74732983e-01, 9.74793930e-01,\n",
      "       9.75114726e-01, 9.76409602e-01, 9.76690437e-01, 9.78120099e-01,\n",
      "       9.80162606e-01, 9.80337937e-01, 9.80359762e-01, 9.80737492e-01,\n",
      "       9.81883572e-01, 9.83840748e-01, 9.84509853e-01, 9.87336079e-01,\n",
      "       9.87562142e-01, 9.87711686e-01, 9.88284120e-01, 9.89510287e-01,\n",
      "       9.89801639e-01, 9.90273611e-01, 9.90425472e-01, 9.91184314e-01,\n",
      "       9.91335445e-01, 9.91982972e-01, 9.92059909e-01, 9.92124743e-01,\n",
      "       9.92145488e-01, 9.92895520e-01, 9.93471526e-01, 9.93608498e-01,\n",
      "       9.93670461e-01, 9.93793101e-01, 9.93799014e-01, 9.94231886e-01,\n",
      "       9.94234153e-01, 9.94534241e-01, 9.95437832e-01, 9.95634633e-01,\n",
      "       9.95754783e-01, 9.95762099e-01, 9.96089276e-01, 9.96652790e-01,\n",
      "       9.96967369e-01, 9.96984119e-01, 9.97204035e-01, 9.97208354e-01,\n",
      "       9.97241107e-01, 9.97254184e-01, 9.97356192e-01, 9.97357711e-01,\n",
      "       9.97666798e-01, 9.97746955e-01, 9.97926669e-01, 9.98026951e-01,\n",
      "       9.98086355e-01, 9.98178313e-01, 9.98200413e-01, 9.98321567e-01,\n",
      "       9.98813672e-01, 9.98819705e-01, 9.98827528e-01, 9.98848154e-01,\n",
      "       9.98888334e-01, 9.98945025e-01, 9.99023317e-01, 9.99105277e-01,\n",
      "       9.99196156e-01, 9.99380578e-01, 9.99519303e-01, 9.99526343e-01]), array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "       1, 1], dtype=int64))\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Found input variables with inconsistent numbers of samples: [320, 80]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-137-8553040e06dc>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;31m#pred_p_bad\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munique\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpred_p_bad\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreturn_counts\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconfusion_matrix\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpred_p_bad\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test_bad\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mclassification_report\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_test_bad\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpred_p_bad\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdigits\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py\u001b[0m in \u001b[0;36mconfusion_matrix\u001b[1;34m(y_true, y_pred, labels, sample_weight)\u001b[0m\n\u001b[0;32m    251\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    252\u001b[0m     \"\"\"\n\u001b[1;32m--> 253\u001b[1;33m     \u001b[0my_type\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_true\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_pred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_check_targets\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    254\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0my_type\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;34m\"binary\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"multiclass\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    255\u001b[0m         \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"%s is not supported\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0my_type\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py\u001b[0m in \u001b[0;36m_check_targets\u001b[1;34m(y_true, y_pred)\u001b[0m\n\u001b[0;32m     69\u001b[0m     \u001b[0my_pred\u001b[0m \u001b[1;33m:\u001b[0m \u001b[0marray\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mindicator\u001b[0m \u001b[0mmatrix\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     70\u001b[0m     \"\"\"\n\u001b[1;32m---> 71\u001b[1;33m     \u001b[0mcheck_consistent_length\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     72\u001b[0m     \u001b[0mtype_true\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtype_of_target\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     73\u001b[0m     \u001b[0mtype_pred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtype_of_target\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36mcheck_consistent_length\u001b[1;34m(*arrays)\u001b[0m\n\u001b[0;32m    203\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0muniques\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    204\u001b[0m         raise ValueError(\"Found input variables with inconsistent numbers of\"\n\u001b[1;32m--> 205\u001b[1;33m                          \" samples: %r\" % [int(l) for l in lengths])\n\u001b[0m\u001b[0;32m    206\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    207\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Found input variables with inconsistent numbers of samples: [320, 80]"
     ]
    }
   ],
   "source": [
    "lda = LDA()\n",
    "model_bad = lda.fit(X_train, y_train_bad)\n",
    "pred_p_bad = model_bad.predict_proba(X_train)\n",
    "\n",
    "#pred_p_bad\n",
    "print(np.unique(pred_p_bad, return_counts=True))\n",
    "print(confusion_matrix(pred_p_bad, y_test_bad))\n",
    "print(classification_report(y_test_bad, pred_p_bad, digits=3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
